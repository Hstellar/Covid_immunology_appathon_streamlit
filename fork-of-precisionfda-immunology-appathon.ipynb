{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install dask\n!pip install sklearn","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import dask.dataframe as dd\nimport pandas as pd\npd.options.display.max_columns = 999\nimport glob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport re\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmetadata = pd.read_csv('data/AdaptiveISB_Metadata.csv', usecols = ['repertoire_id', 'sex', 'age_min', 'ethnicity',\n       'disease_length', 'disease_stage', 'intervention', 'medical_history',\n       'collection_time_point_relative'])\nmetadata.collection_time_point_relative = metadata.collection_time_point_relative.str.rsplit(\" d\", n=0, expand=True).rename(columns = {0:'collection_time_point_relative'}).drop(1, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data = dd.read_csv('data/adaptive-ISB-combined.tsv',sep='\\t')\nraw_data=raw_data.repartition(partition_size=\"20GB\")\nraw_data.to_parquet('data/adaptive_combined.parquet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### V call"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"%%time\ndf1 = pd.DataFrame()\nfor parq_file in glob.glob('data/adaptive_combined/part.*.parquet'):\n    \n    data = pd.read_parquet(parq_file,columns=['v_call','repertoire_id']).drop_duplicates().reset_index(drop=True)\n    count_vec = CountVectorizer( analyzer='word', tokenizer=lambda x: re.split(r', |or\\s', x),\n                                ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n    count_train = count_vec.fit(data.v_call)\n    bag_of_words = count_vec.transform(data.v_call)\n    rr = pd.DataFrame(bag_of_words.toarray(), columns = count_vec.get_feature_names()).drop([''],axis=1)\n    cols = rr.columns\n    print(parq_file)\n    df = pd.concat([data[['repertoire_id']],rr],axis=1)\n    df = df.groupby('repertoire_id')[cols].sum().reset_index()\n    df1 = pd.concat([df1, df], sort=False).fillna(0)\n    cols = df1.drop('repertoire_id',axis=1).columns\n    df1 = df1.groupby('repertoire_id')[cols].sum().reset_index()\n    del data\n    del bag_of_words\n    del rr\n    del df\ndf2.to_parquet('data/adaptiveisb1_v_call.parquet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndata = pd.read_parquet('data/adaptiveisb1_v_call.parquet')\ncols = data.drop('repertoire_id',axis=1).columns\ndata = data.groupby('repertoire_id')[cols].sum().reset_index()\ndff = pd.merge(metadata, data, on='repertoire_id', how='inner')\ndff.to_parquet('data/v_call.parquet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# D_call"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"%%time\ndf2 = pd.DataFrame()\nfor parq_file in glob.glob('data/adaptive_combined/part.*.parquet'):\n    \n    data = pd.read_parquet(parq_file,columns=['d_call','repertoire_id']).drop_duplicates().reset_index(drop=True)\n    count_vec = CountVectorizer( analyzer='word', tokenizer=lambda x: re.split(r', |or\\s', x),\n                                ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n    data.d_call.fillna(value='None', inplace=True)\n    count_train = count_vec.fit(data.d_call)\n    bag_of_words = count_vec.transform(data.d_call)\n    rr = pd.DataFrame(bag_of_words.toarray(), columns = count_vec.get_feature_names()).drop(['','none'],axis=1)\n    cols = rr.columns\n    print(parq_file)\n    df = pd.concat([data[['repertoire_id']],rr],axis=1)\n    df = df.groupby('repertoire_id')[cols].sum().reset_index()\n    df2 = pd.concat([df2, df], sort=False).fillna(0)\n    cols = df2.drop('repertoire_id',axis=1).columns\n    df2 = df2.groupby('repertoire_id')[cols].sum().reset_index()\n    del data\n    del bag_of_words\n    del rr\n    del df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.to_parquet('data/adaptiveisb2_d_call.parquet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_parquet('data/adaptiveisb2_d_call.parquet')\ncols = data.drop('repertoire_id',axis=1).columns\ndata = data.groupby('repertoire_id')[cols].sum().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dff = pd.merge(metadata, data, on='repertoire_id', how='inner')\ndff.to_parquet('data/d_call.parquet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# J_call"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"%%time\ndf2 = pd.DataFrame()\nfor parq_file in glob.glob('data/adaptive_combined/part.*.parquet'):\n    \n    data = pd.read_parquet(parq_file,columns=['j_call','repertoire_id']).drop_duplicates().reset_index(drop=True)\n    count_vec = CountVectorizer( analyzer='word', tokenizer=lambda x: re.split(r', |or\\s', x),\n                                ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n    data.j_call.fillna(value='None', inplace=True)\n    count_train = count_vec.fit(data.j_call)\n    bag_of_words = count_vec.transform(data.j_call)\n    rr = pd.DataFrame(bag_of_words.toarray(), columns = count_vec.get_feature_names()).drop([''],axis=1)\n    cols = rr.columns\n    print(parq_file)\n    df = pd.concat([data[['repertoire_id']],rr],axis=1)\n    df = df.groupby('repertoire_id')[cols].sum().reset_index()\n    df2 = pd.concat([df2, df], sort=False).fillna(0)\n    cols = df2.drop('repertoire_id',axis=1).columns\n    df2 = df2.groupby('repertoire_id')[cols].sum().reset_index()\n    del data\n    del bag_of_words\n    del rr\n    del df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.to_parquet('data/adaptiveisb2_j_call.parquet.parquet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_parquet('data/adaptiveisb2_j_call.parquet.parquet')\ncols = data.drop('repertoire_id',axis=1).columns\ndata = data.groupby('repertoire_id')[cols].sum().reset_index()\ndff = pd.merge(metadata, data, on='repertoire_id', how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dff.to_parquet('data/j_call.parquet')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}